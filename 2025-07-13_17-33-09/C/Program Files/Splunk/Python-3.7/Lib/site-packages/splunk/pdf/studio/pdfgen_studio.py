import copy
import json
import time
from builtins import range
from threading import Thread
from queue import Queue
import re
import uuid

from splunk import AuthorizationFailed
import splunk.pdf.availability as pdf_availability
import splunk.pdf.pdfgen_utils as pdfgen_utils
import splunk.entity as entity
import splunk.search as search
import splunk.rest as rest
import splunk.safe_lxml_etree as et
from splunk.pdf.pdfgen_search import InlineSearchManager, SavedSearchManager, PostProcessSearchManager
import splunk.util
from splunk.rest.splunk_web_requests import get_dashboards_csp_settings
from splunk.pdf.studio.url_utils import getDataURIForExternalImage, extractCSPAndTrustedDomains
from splunk.pdf.studio.visual_exporter import get_screenshot

logger = pdfgen_utils.getLogger()
DEFAULT_TIMEOUT = 3600
READ_ONLY_TOKEN_NAMESPACES = {'env'}
MAX_CONCURRENT_SEARCHES = 3
REPLACE_EXTERNAL_IMAGES_WITH_DATA_URIS = True

FEATURE_FLAG_ERROR_MSG = 'Set the activate_scheduled_export feature flag to render the dashboard %s to PDF.'
VISUAL_EXPORTER_ERROR_MSG = 'Install or turn on the Splunk Visual Exporter app to render the dashboard %s to PDF.'
INVALID_DEFINITION_ERROR_MSG = 'Invalid dashboard definition. Correct errors in the dashboard definition to render the dashboard %s to PDF.'

def parseDataSourceSearchResults(results, fieldOrder):
    """
    Parses ResultSet results and fieldOrder into fields and columns format. Do not include fields from fieldOrder
    if there are no columns. Modelled after splunk.pdfgen_table.addRowFromSearchResult()
    """
    parsedResults = {}
    for i, resultRow in enumerate(results):
        for field in resultRow.fields:
            existingFieldValues = parsedResults.get(field, [])
            newFieldValues = resultRow[field]
            # make new field values all strings and append to existingFieldValues to add to parsedResults
            if isinstance(newFieldValues, search.RawEvent):
                fieldValuesStr = newFieldValues
                existingFieldValues.append(str(fieldValuesStr))
            elif len(newFieldValues) > 1:
                fieldValueStrings = [str(x) for x in newFieldValues]
                existingFieldValues.append(fieldValueStrings)
            else:
                fieldValuesStr = newFieldValues[0]
                existingFieldValues.append(str(fieldValuesStr))
            parsedResults[field] = existingFieldValues
    columns = []
    fields = []
    keys = parsedResults.keys()
    for field in fieldOrder:
        if field in keys:
            columnValues = parsedResults[field]
            if len(columnValues) > 0:
                columns.append(columnValues)
                fields.append(field)
    return fields, columns


def getDataSourceSearchResults(searchManager, resolvedDataSources, erroredDataSources, ds, checkRiskyCommand, dataSourceDef):
    """
    Dispatches searchManager to retrieve search results for ds in fields/columns format. If the search completes
    successfully, the dataSource is added to resolvedDataSources with the corresponding results stored in a ds.test
    format. If there is an error, the dataSource is added to erroredDataSources with the corresponding error message.
    """
    logger.debug("getting search results for dataSource %s" % ds)
    if searchManager is None:
        erroredDataSources[ds] = 'Unable to create search.'
        return

    timeoutTime = int(time.time()) + DEFAULT_TIMEOUT
    try:
        searchManager.dispatch(checkRiskyCommand=checkRiskyCommand)
        while not searchManager.isComplete() and not searchManager.isRealtime():
            time.sleep(1)
            currentTime = int(time.time())
            timedOut = currentTime > timeoutTime
            if timedOut:
                searchManager.cancel()
                erroredDataSources[ds] = 'Search timed out after %s seconds.' % DEFAULT_TIMEOUT
                return

        results = searchManager.results()
        fieldOrder = results.fieldOrder
        fields, columns = parseDataSourceSearchResults(results, fieldOrder)

        # modify data obj to contain search results as ds.test
        dsTest = {
            'type': 'ds.test',
            'options': {
                'data': {
                    'fields': fields,
                    'columns': columns
                },
                'meta': {},
            }
        }
        dataSourceName = dataSourceDef.get('name')
        if dataSourceName is not None:
            dsTest['name'] = dataSourceName
        enableSmartSources = dataSourceDef.get('options', {}).get(
            'enableSmartSources')
        if enableSmartSources is not None:
            dsTest['options']['enableSmartSources'] = enableSmartSources 
        resolvedDataSources[ds] = dsTest
    except Exception as e:
        if isinstance(e, AuthorizationFailed) and checkRiskyCommand and 'check_risky_command' in e.extendedMessages:
            # This is a special case for chain searches containing risky commands, which throw an AuthorizationFailed
            # exception which does not contain any useful info for security reasons. Because of this, we need to return
            # a more generic risky commands error message.
            errorMessage = 'Found potentially risky commands.'
        else:
            errorMessage = str(e)
        erroredDataSources[ds] = errorMessage
        return


def getDataURI(url, collection, sessionKey, includeExternal=False, trustedDomains=None):
    """
    return the data URI for a given url, if there is an error accessing the url, return None.
    If the url is not trusted, then return None
    """
    if url is None:
        return None

    kvstoreIdentifier = 'splunk-enterprise-kvstore://'
    if not url.startswith(kvstoreIdentifier):
        if includeExternal:
            return getDataURIForExternalImage(url, trustedDomains)
        return None
    try:
        key = url.split(kvstoreIdentifier)[1]
        uri = entity.buildEndpoint(
            ['storage', 'collections', 'data', collection, key],
            namespace='splunk-dashboard-studio'
        )
        jobResponseHeaders, jobResponseBody = rest.simpleRequest(
            uri, method='GET', sessionKey=sessionKey, getargs={'output_mode': 'json'})
        searchJob = json.loads(jobResponseBody)
        return searchJob.get('dataURI')
    except Exception:
        return None


def getSearchManager(dataSourceName, dataSources, managerMap, namespace, owner, sessionKey):
    """
    Return the search manager for the corresponding dataSourceName if it already exists in the managerMap. Otherwise,
    create and return a search manager based on the type of dataSource, or None if there is an error.
    Add the search manager to the managerMap.
    """
    if dataSourceName in managerMap:
        return managerMap[dataSourceName]

    searchManager = None
    dataSource = dataSources.get(dataSourceName, {})
    options = dataSource.get('options', {})
    type = dataSource.get('type')

    if type == 'ds.search':
        query = options.get('query')
        if query is not None:
            earliest = options.get('queryParameters', {}).get('earliest')
            latest = options.get('queryParameters', {}).get('latest')
            searchManager = InlineSearchManager(
                query, earliest, latest, namespace, owner, sessionKey)
    elif type == 'ds.savedSearch':
        name = options.get('ref')
        if name is not None:
            searchManager = SavedSearchManager(
                name, namespace=namespace, owner=owner, sessionKey=sessionKey)
    elif type == 'ds.chain':
        query = options.get('query')
        parentDataSourceName = options.get('extend')
        if query is not None and parentDataSourceName is not None:
            parentSearchManager = getSearchManager(
                parentDataSourceName, dataSources, managerMap, namespace, owner, sessionKey)
            if parentSearchManager is not None:
                searchManager = PostProcessSearchManager(
                    query, parentSearchManager, namespace, owner, sessionKey)

    if searchManager is None:
        logger.error(
            'Unable to create search manager for dataSource %s.' % dataSourceName)

    managerMap[dataSourceName] = searchManager
    return searchManager


def getInputValues(definition):
    """
    Create a dictionary of input values and their default definitions. If no default definition exists, the input
    is not added. If the input is a time range, separate the earliest and latest values in the dictionary.
    """
    inputs = definition.get('inputs', {})
    inputValues = {}
    for input in inputs:
        inputType = inputs[input].get('type')
        defaultValue = inputs[input].get('options', {}).get('defaultValue')
        tokenName = inputs[input].get('options', {}).get('token')
        tokenNamespace = inputs[input].get('options', {}).get('tokenNamespace')
        # tokens in readonly namespaces shouldn't be modifiable by inputs
        if tokenNamespace not in READ_ONLY_TOKEN_NAMESPACES:
            if tokenName is not None and defaultValue is not None:
                if tokenNamespace is not None:
                    tokenName = '{}:{}'.format(tokenNamespace, tokenName)
                if inputType == 'input.timerange':
                    inputValues[tokenName +
                                '.earliest'] = defaultValue.split(',')[0].strip()
                    inputValues[tokenName +
                                '.latest'] = defaultValue.split(',')[1].strip()
                else:
                    inputValues[tokenName] = defaultValue
    return inputValues


def hasToken(string):
    """
    Return true if string contains any tokens.
    That is, any character(s) that start and end with a $ character
    """
    return re.search(r'(\$([\w:\.]+?)\$)', string)


def mergeOptions(options1, options2):
    """
    Return new object with merged options1 and options2, where options1 takes precedence
    """
    mergedOptions = copy.deepcopy(options2)
    mergedOptions.update(options1)
    return mergedOptions


def getMergedDefaultOptionsForDataSourceType(defaults, dataSourceType):
    """
    Return a merged set of global and type specific default options for the given data source type.
    Type specific default options take precedence over global default options. Exclude query from these options.
    """
    if defaults == {} or dataSourceType is None:
        return {}

    globalOptions = defaults.get('dataSources', {}).get(
        'global', {}).get('options', {})
    typeSpecificOptions = defaults.get('dataSources', {}).get(
        dataSourceType, {}).get('options', {})
    mergedOptions = mergeOptions(typeSpecificOptions, globalOptions)
    if mergedOptions.get("query") is not None:
        del mergedOptions["query"]
    return mergedOptions


def getMergedOptionsForDataSource(dataSource, mergedDefaultOptions, defaults, dsName):
    """
    Return a merged set of default and data source specific options for the given data source.
    Data source specific options take precedence over default options.
    """
    logger.debug("getting merged options for dataSource %s" % dsName)
    dataSourceType = dataSource.get('type')
    dataSourceOptions = dataSource.get('options', {})
    if not mergedDefaultOptions.get(dataSourceType, {}):
        mergedDefaultOptions[dataSourceType] = getMergedDefaultOptionsForDataSourceType(
            defaults, dataSourceType)
    defaultOptions = mergedDefaultOptions[dataSourceType]
    return mergeOptions(dataSourceOptions, defaultOptions)


def runSearches(searchQueue, resolvedDataSources, erroredDataSources, checkRiskyCommand):
    """
    Runs all the searches in the searchQueue
    """
    while not searchQueue.empty():
        searchManager, datasourceId, dataSourceDef = searchQueue.get()
        getDataSourceSearchResults(
            searchManager, resolvedDataSources, erroredDataSources, 
            datasourceId, checkRiskyCommand, dataSourceDef)
        searchQueue.task_done()


def modifyDataSourceSearches(definition, namespace, owner, sessionKey):
    """
    For every dataSource in the definition, run the search and replace it with the results as ds.test.
    The original definition is modified. If a dataSource search results in an error, the dataSource name 
    is added to the erroredDataSources dictionary, with the corresponding error message.
    """
    logger.debug("modifying dataSource searches")
    dataSources = definition.get('dataSources', {})
    dataSourcesCopy = copy.deepcopy(dataSources)
    defaults = definition.get('defaults', {})
    mergedDefaultOptions = {}
    managerMap = {}
    erroredDataSources = {}
    resolvedDataSources = {}
    searchQueue = Queue()
    checkRiskyCommand = pdfgen_utils.isRiskyCommandCheckDashboardEnabled(
        sessionKey)

    for ds in dataSourcesCopy:
        dataSource = dataSources[ds]
        dataSource['options'] = getMergedOptionsForDataSource(
            dataSource, mergedDefaultOptions, defaults, ds)
        if hasToken(json.dumps(dataSource)):
            errorMessage = "Set token value to render visualization"
            erroredDataSources[ds] = errorMessage
        elif dataSource.get('type') != 'ds.test':
            searchManager = getSearchManager(
                ds, dataSources, managerMap, namespace, owner, sessionKey)
            searchQueue.put((searchManager, ds, dataSource))
    for _ in range(MAX_CONCURRENT_SEARCHES):
        thread = Thread(target=runSearches, args=[
                        searchQueue, resolvedDataSources, erroredDataSources, checkRiskyCommand])
        thread.start()
    searchQueue.join()
    for ds in resolvedDataSources:
        logger.debug('Resolving datasource %s' % ds)
        dataSources[ds] = resolvedDataSources[ds]
    for ds in erroredDataSources:
        errorMessage = erroredDataSources[ds]
        logger.error("Error for DataSource %s: %s" % (ds, errorMessage))
        dataSources[ds] = _createDsTestWithError(errorMessage)
    logger.debug(definition)
    return erroredDataSources

def _createDsTestWithError(errorMessage):
    """
    Return a ds.test with a given error message
    """
    return {
        'type': 'ds.test',
        'options': {
            'error': errorMessage
        }
    }

def generateRandomId(idPrefix):
    """
    Generate a random id in the format of idPrefix_<random_string>
    """
    return '%s_%s' % (idPrefix, str(uuid.uuid4()))

def replaceVizDataSourceWithError(definition, vizName, errorMessage):
    """
    Replace a visualization's data source with a ds.test containing a given
    error message
    """
    visualization = definition['visualizations'][vizName]
    dsTest = _createDsTestWithError(errorMessage)
    if 'dataSources' in definition:
        dataSources = definition['dataSources']
    else:
        dataSources = {}
        definition['dataSources'] = dataSources
    
    dsId = generateRandomId('ds')
    definition['dataSources'][dsId] = dsTest
    visualization['dataSources'] = {
        'primary': dsId
    }

def modifyBackgroundImageDataURI(definition, sessionKey, includeExternal=False, trustedDomains=None):
    """
    If there is a backgroundImage, replace the URL with the data URI. If there
    is an error retrieving the data URI, delete the backgroundImage from layout options.
    """
    logger.debug('replacing background image URL with data URI')
    if definition.get('layout', {}).get('options', {}).get('backgroundImage'):
        url = definition['layout']['options']['backgroundImage'].get('src')
        dataURI = getDataURI(url, 'splunk-dashboard-images',
                             sessionKey, includeExternal, trustedDomains)
        if dataURI:
            definition['layout']['options']['backgroundImage']['src'] = dataURI
        else:
            del definition['layout']['options']['backgroundImage']


def modifyVizDataURI(definition, vizName, sessionKey, includeExternal=False, trustedDomains=None):
    """
    For image or singlevalueicon visualizations, replace the URL with the data URI.
    If there is an error retrieving the data URI, convert the visualization to viz.error.
    """
    logger.debug(
        "replacing image/icon URL with data URI for visualization %s" % vizName)
    viz = definition['visualizations'][vizName]
    vizType = viz.get('type')
    if vizType == 'viz.img' or vizType == 'splunk.image':
        url = viz.get('options', {}).get('src')
        if url is None:
            return
        dataURI = getDataURI(url, 'splunk-dashboard-images',
                             sessionKey, includeExternal, trustedDomains)
        if dataURI:
            definition['visualizations'][vizName]['options']['src'] = dataURI
        else:
            replaceVizDataSourceWithError(definition, vizName, 'Image not found')
            del definition['visualizations'][vizName]['options']['src']

    elif vizType == 'viz.singlevalueicon' or vizType == 'splunk.singlevalueicon':
        url = viz.get('options', {}).get('icon')
        if url is None:
            return
        dataURI = getDataURI(url, 'splunk-dashboard-icons',
                             sessionKey, includeExternal, trustedDomains)
        if dataURI:
            definition['visualizations'][vizName]['options']['icon'] = dataURI
        else:
            del definition['visualizations'][vizName]['options']['icon']


def getServerInfo(sessionKey):
    return entity.getEntity('/server', 'info', sessionKey=sessionKey)


def getUserInfo(sessionKey):
    return entity.getEntity('/authentication/', 'current-context', sessionKey=sessionKey)


def getEnvironmentTokens(sessionKey, namespace, dashboardName):
    """
    Gets standard environment tokens as mentioned here excluding locale
    https://docs.splunk.com/Documentation/Splunk/8.2.1/Viz/tokens#Use_global_tokens_to_access_environment_information
    """
    environment_tokens = {'env:app': namespace, 'env:page': dashboardName}
    try:
        serverInfo = getServerInfo(sessionKey)
        product_type = serverInfo.get('product_type')
        environment_tokens['env:product'] = product_type
        environment_tokens['env:version'] = serverInfo.get('version')

        is_product_types = ['enterprise', 'hunk', 'lite', 'lite_free']
        if product_type in is_product_types:
            environment_tokens['env:is_{}'.format(product_type)] = 'true'

        instanceType = serverInfo.get('instance_type')
        if instanceType:
            environment_tokens['env:instance_type'] = instanceType
            if instanceType == 'cloud':
                environment_tokens['env:is_cloud'] = 'true'

        if splunk.util.normalizeBoolean(serverInfo.get('isFree')):
            environment_tokens['env:is_free'] = 'true'

        user = getUserInfo(sessionKey)
        environment_tokens['env:user'] = user.get('username')
        environment_tokens['env:user_realname'] = user.get('realname')
        environment_tokens['env:user_email'] = user.get('email')

    except Exception as e:
        logger.error('Exception while fetching environment tokens: %s', str(e))

    return environment_tokens


def replaceTokens(definition, sessionKey, namespace, dashboardName):
    """
    Replace tokens in dataSources, defaults, and visualizations with the default values of tokens
    defined in the definition. Modifies the original definition.
    Note that in the following example, a token with an int type will remain an int
        definition = { "count": "$token$" }
    Whereas in the following example, a token with an int type will be converted to a string
        definition = { "query": "index = $token$" }
    """
    tokenValues = {}
    tokenValues.update(getInputValues(definition))
    tokenValues.update(getEnvironmentTokens(
        sessionKey, namespace, dashboardName))

    def _replaceTokensInValue(value):
        tokensInValue = re.findall(r'\$([^$]+?)\$', value)
        for token in tokensInValue:
            tokenString = '$' + token + '$'
            tokenValue = tokenValues.get(token)
            if tokenValue is None:
                continue
            if type(tokenValue) is int:
                if value == tokenString:
                    return tokenValue
                tokenValue = str(tokenValue)
            value = value.replace(tokenString, tokenValue)
        return value

    maxRecursionLevel = 10  # Same limit as in UDF

    def _replaceTokens(definition, recursionLevel=0):
        if isinstance(definition, str):
            definition = _replaceTokensInValue(definition)
        elif isinstance(definition, list) and recursionLevel <= maxRecursionLevel:
            for i, value in enumerate(definition):
                definition[i] = _replaceTokens(value, recursionLevel + 1)
        elif isinstance(definition, dict) and recursionLevel <= maxRecursionLevel:
            for key, value in definition.items():
                definition[key] = _replaceTokens(value, recursionLevel + 1)
        return definition

    _replaceTokens(definition)

def checkVisualExporterInstalledAndEnabled(sessionKey):
    """
    Check that Splunk Visual Exporter is installed by calling services/apps/local/splunk-visual-exporter
    """
    try:
        response, content = rest.simpleRequest(
            'apps/local/splunk-visual-exporter', sessionKey=sessionKey, getargs={'output_mode': 'json'})
        appResponse = json.loads(content)
        isDisabled = appResponse.get('entry', [{}])[0].get('content', {}).get('disabled')
        return not isDisabled
    except Exception:
        return False

def convertDashboardToPdfContent(sessionKey, namespace, owner, studioDefinition, dashboardName, studioErrors):
    """
    Convert the given definition to pdf content. Return False if service is not enabled or available.
    """
    # ensure correct permissions and services are enabled and available
    isScheduledExportEnabled = pdf_availability.is_studio_dashboard_scheduled_export_enabled(
        sessionKey)
    if isScheduledExportEnabled is not True:
        logger.info('activate_scheduled_export setting in web-features.conf is False')
        studioErrors.append(FEATURE_FLAG_ERROR_MSG % dashboardName)
        return False
    
    # ensure that Splunk Visual Exporter is installed and enabled
    isAppInstalledAndEnabled = checkVisualExporterInstalledAndEnabled(sessionKey)
    if isAppInstalledAndEnabled is not True:
        logger.info('Splunk Visual Exporter is either not installed or is disabled')
        studioErrors.append(VISUAL_EXPORTER_ERROR_MSG % dashboardName)
        return False

    # get CSP header and trusted domains
    csp_header, trustedDomains = extractCSPAndTrustedDomains(
        get_dashboards_csp_settings(sessionKey))

    # convert dashboard definition from xml to json
    xmlDefinition = et.fromstring(studioDefinition)
    theme = xmlDefinition.attrib['theme'] if 'theme' in xmlDefinition.attrib else 'light'
    stringDefinition = xmlDefinition.find('definition').text
    try:
        definition = json.loads(stringDefinition)
    except json.JSONDecodeError:
        logger.exception('Invalid dashboard definition')
        studioErrors.append(INVALID_DEFINITION_ERROR_MSG % dashboardName)
        return False
    
    # modify definition to prepare it for conversion to pdf content
    replaceTokens(definition, sessionKey, namespace, dashboardName)
    modifyDataSourceSearches(
        definition, namespace, owner, sessionKey)
    visualizations = definition.get('visualizations', {})
    for vizName in visualizations:
        modifyVizDataURI(definition, vizName, sessionKey,
                            REPLACE_EXTERNAL_IMAGES_WITH_DATA_URIS, trustedDomains)
    modifyBackgroundImageDataURI(
        definition, sessionKey, REPLACE_EXTERNAL_IMAGES_WITH_DATA_URIS, trustedDomains)

    return get_screenshot(definition, theme, sessionKey, studioErrors)
